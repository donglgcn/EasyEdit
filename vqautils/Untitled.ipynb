{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c4d095-82aa-464e-b123-3de8b4f21f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /localtmp/ktm8eh/miniconda3\n",
      "MMEDIT                *  /localtmp/ktm8eh/miniconda3/envs/MMEDIT\n",
      "RL                       /localtmp/ktm8eh/miniconda3/envs/RL\n",
      "TableText                /localtmp/ktm8eh/miniconda3/envs/TableText\n",
      "VAD                      /localtmp/ktm8eh/miniconda3/envs/VAD\n",
      "dassl                    /localtmp/ktm8eh/miniconda3/envs/dassl\n",
      "editing                  /localtmp/ktm8eh/miniconda3/envs/editing\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f433d0-6473-4950-a3b1-122bd52fc56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "creating index...\n",
      "index created!\n",
      "Question: What building is this?\n",
      "Answer 1: church\n",
      "Answer 2: church\n",
      "Answer 3: church\n",
      "Answer 4: church\n",
      "Answer 5: church\n",
      "Answer 6: church\n",
      "Answer 7: church\n",
      "Answer 8: church\n",
      "Answer 9: church\n",
      "Answer 10: church\n",
      "Figure(640x480)\n",
      "Loading VIT\n",
      "Position interpolate from 16x16 to 26x26\n",
      "\u001b[39mfreeze vision encoder\u001b[0m\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'bert.embeddings.token_type_embeddings.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.0.intermediate_query.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[39mload checkpoint from ../hugging_cache/blip2_pretrained_flant5xxl.pth\u001b[0m\n",
      "\u001b[39mfreeze Qformer\u001b[0m\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:22<00:00,  7.60s/it]\n",
      "Loading LLAMA Done\n",
      "Traceback (most recent call last):\n",
      "  File \"/localtmp/ktm8eh/dongliang/code/EasyEdit/vqautils/./vqa_test.py\", line 76, in <module>\n",
      "    ans = model.predict_answers(samples={\"image\": image, \"text_input\": question}, inference_method=\"generate\")\n",
      "  File \"/localtmp/ktm8eh/dongliang/code/EasyEdit/vqautils/../easyeditor/trainer/blip2_models/mini_gpt4.py\", line 283, in predict_answers\n",
      "    img_embeds, atts_img = self.encode_img(image)\n",
      "  File \"/localtmp/ktm8eh/dongliang/code/EasyEdit/vqautils/../easyeditor/trainer/blip2_models/mini_gpt4.py\", line 147, in encode_img\n",
      "    image_embeds = self.ln_vision(self.visual_encoder(image)).to(device)\n",
      "  File \"/localtmp/ktm8eh/miniconda3/envs/MMEDIT/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/localtmp/ktm8eh/dongliang/code/EasyEdit/vqautils/../easyeditor/trainer/blip2_models/eva_vit.py\", line 350, in forward\n",
      "    x = self.forward_features(x)\n",
      "  File \"/localtmp/ktm8eh/dongliang/code/EasyEdit/vqautils/../easyeditor/trainer/blip2_models/eva_vit.py\", line 325, in forward_features\n",
      "    x = self.patch_embed(x)\n",
      "  File \"/localtmp/ktm8eh/miniconda3/envs/MMEDIT/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/localtmp/ktm8eh/dongliang/code/EasyEdit/vqautils/../easyeditor/trainer/blip2_models/eva_vit.py\", line 203, in forward\n",
      "    x = self.proj(x).flatten(2).transpose(1, 2)\n",
      "  File \"/localtmp/ktm8eh/miniconda3/envs/MMEDIT/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/localtmp/ktm8eh/miniconda3/envs/MMEDIT/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/localtmp/ktm8eh/miniconda3/envs/MMEDIT/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Input type (float) and bias type (c10::Half) should be the same\n"
     ]
    }
   ],
   "source": [
    "!python ./vqa_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9bdb64-ccc0-4b5e-8f43-253a5f58fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import sys\n",
    "sys.path.append(\"../easyeditor/dataset/processor/\")\n",
    "sys.path.append(\"../\")\n",
    "from blip_processors import BlipImageEvalProcessor\n",
    "from easyeditor import MiniGPT4\n",
    "from vqa import VQA\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769973d6-ca42-4b6e-80a4-02f96f9b8379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT\n",
      "Position interpolate from 16x16 to 26x26\n",
      "freeze vision encoder\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['bert.embeddings.token_type_embeddings.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.9.output_query.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.5.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.7.output_query.dense.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.output_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.weight', 'bert.encoder.layer.4.output_query.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.9.output_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.9.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.11.output_query.LayerNorm.weight', 'bert.encoder.layer.9.intermediate_query.dense.weight', 'bert.encoder.layer.10.output_query.dense.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.9.intermediate_query.dense.bias', 'bert.encoder.layer.8.output_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.7.intermediate_query.dense.bias', 'bert.encoder.layer.5.intermediate_query.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate_query.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.11.output_query.dense.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.5.intermediate_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.11.output_query.LayerNorm.bias', 'bert.encoder.layer.7.output_query.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.6.output_query.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.6.output_query.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate_query.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.9.output_query.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.output_query.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.output_query.dense.weight', 'bert.encoder.layer.11.output_query.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.11.intermediate_query.dense.bias', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.intermediate_query.dense.bias', 'bert.encoder.layer.8.output_query.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.5.output_query.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.7.intermediate_query.dense.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.7.output_query.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.11.intermediate_query.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.output_query.LayerNorm.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.8.intermediate_query.dense.weight', 'bert.encoder.layer.6.intermediate_query.dense.bias', 'bert.encoder.layer.2.output_query.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from ../hugging_cache/blip2_pretrained_flant5xxl.pth\n",
      "freeze Qformer\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007371425628662109,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c9824537b5497485bdb296407e48d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n"
     ]
    }
   ],
   "source": [
    "from easyeditor import MiniGPT4\n",
    "qformer_checkpoint= \"../hugging_cache/blip2_pretrained_flant5xxl.pth\"\n",
    "qformer_name_or_path= \"bert-base-uncased\"\n",
    "state_dict_file= \"../hugging_cache/eva_vit_g.pth\"\n",
    "name= \"../hugging_cache/vicuna-7b\"\n",
    "model_name= \"minigpt4\"\n",
    "model = MiniGPT4(\n",
    "    vit_model=\"eva_clip_g\",\n",
    "    q_former_model=qformer_checkpoint,\n",
    "    img_size=364,\n",
    "    use_grad_checkpoint=True,\n",
    "    vit_precision=\"fp32\",\n",
    "    freeze_vit=True,\n",
    "    llama_model=name,\n",
    "    state_dict_file=state_dict_file,\n",
    "    qformer_name_or_path=qformer_name_or_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a27e2654-e975-4228-847d-0bf035e225c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a173725d-89bf-4019-8d7d-706b09907475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "creating index...\n",
      "index created!\n",
      "Question: Name the type of house where these windows are seen?\n",
      "Answer 1: jail\n",
      "Answer 2: jail\n",
      "Answer 3: jail\n",
      "Answer 4: jail\n",
      "Answer 5: jail\n",
      "Answer 6: jail\n",
      "Answer 7: beach house\n",
      "Answer 8: beach house\n",
      "Answer 9: old\n",
      "Answer 10: old\n"
     ]
    }
   ],
   "source": [
    "ROOT\t\t='/localtmp/ktm8eh/datasets/VQA/okvqa'\n",
    "# versionType ='v2_' # this should be '' when using VQA v2.0 dataset\n",
    "taskType    ='OpenEnded' # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
    "dataSubType ='val2014'\n",
    "annfile = os.path.join(ROOT, 'mscoco_val2014_annotations.json')\n",
    "quesfile = os.path.join(ROOT, 'OpenEnded_mscoco_val2014_questions.json')\n",
    "imgDir \t\t= os.path.join(\"/localtmp/ktm8eh/datasets/mscoco/images\", dataSubType)\n",
    "# initialize VQA api for QA annotations\n",
    "vqa=VQA(annotation_file=annfile, question_file=quesfile)\n",
    "\n",
    "# load and display QA annotations for given question types\n",
    "\"\"\"\n",
    "All possible quesTypes for abstract and mscoco has been provided in respective text files in ../QuestionTypes/ folder.\n",
    "\"\"\"\n",
    "annIds = vqa.getQuesIds(quesTypes='six')\n",
    "anns = vqa.loadQA(annIds)\n",
    "randomAnn = random.choice(anns)\n",
    "vqa.showQA([randomAnn])\n",
    "imgId = randomAnn['image_id']\n",
    "imgFilename = 'COCO_' + dataSubType + '_'+ str(imgId).zfill(12) + '.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b9bbb28-c735-4d3c-b9b9-47fe043accf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_processor = BlipImageEvalProcessor(image_size=364, mean=None, std=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2eb806d-664e-444c-9209-4562bfab02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [\"Which city is this photo taken?\"]\n",
    "raw_image = Image.open(os.path.join(imgDir, imgFilename)).convert(\"RGB\")\n",
    "image = vis_processor(raw_image).unsqueeze(0).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a16d81-f879-442c-9826-abf7f1885b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/localtmp/ktm8eh/miniconda3/envs/MMEDIT/lib/python3.9/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average [unused333] [unused692] [unused305] [unused273]वডளต']\n"
     ]
    }
   ],
   "source": [
    "ans = model.predict_answers(samples={\"image\": image, \"text_input\": question}, inference_method=\"generate\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953e6952-ed1d-4ab3-9614-62543f96a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdbf5ed-3864-4927-a428-055bea5437c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4239512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(prompt_file='mscoco_val2014_annotations.json', partition=0):\n",
    "    with open(prompt_file, 'r') as file:\n",
    "        prompt = json.load(file)\n",
    "    anns = prompt['annotations']\n",
    "    return anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d254fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = load_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3abb402e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297147"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anns[0]['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36978f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5046"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "806c1002",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = [anns[i]['image_id'] for i in range(len(anns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b01365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[572399, 236542, 58472, 191585, 318671, 84643, 536375, 553942, 282037, 519475, 558253, 382307, 55299]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print([item for item, count in collections.Counter(image_ids).items() if count > 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "101f3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find id is 572399\n",
    "ann_ids = [ann for ann in anns if ann['image_id'] == 572399]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ad0de6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 572399,\n",
       "  'answer_type': 'other',\n",
       "  'question_type': 'seven',\n",
       "  'question_id': 5723996,\n",
       "  'answers': [{'answer_id': 1,\n",
       "    'raw_answer': 'man',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'man'},\n",
       "   {'answer_id': 2,\n",
       "    'raw_answer': 'man',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'man'},\n",
       "   {'answer_id': 3,\n",
       "    'raw_answer': 'man',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'man'},\n",
       "   {'answer_id': 4,\n",
       "    'raw_answer': 'man',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'man'},\n",
       "   {'answer_id': 5,\n",
       "    'raw_answer': 'man',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'man'},\n",
       "   {'answer_id': 6,\n",
       "    'raw_answer': 'man',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'man'},\n",
       "   {'answer_id': 7,\n",
       "    'raw_answer': 'men',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'men'},\n",
       "   {'answer_id': 8,\n",
       "    'raw_answer': 'men',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'men'},\n",
       "   {'answer_id': 9,\n",
       "    'raw_answer': 'men',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'men'},\n",
       "   {'answer_id': 10,\n",
       "    'raw_answer': 'men',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'men'}],\n",
       "  'confidence': 3,\n",
       "  'image_object': 'a dirty or unflushed toilet'},\n",
       " {'image_id': 572399,\n",
       "  'answer_type': 'other',\n",
       "  'question_type': 'seven',\n",
       "  'question_id': 5723995,\n",
       "  'answers': [{'answer_id': 1,\n",
       "    'raw_answer': 'down',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'down'},\n",
       "   {'answer_id': 2,\n",
       "    'raw_answer': 'down',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'down'},\n",
       "   {'answer_id': 3,\n",
       "    'raw_answer': 'down',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'down'},\n",
       "   {'answer_id': 4,\n",
       "    'raw_answer': 'down',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'down'},\n",
       "   {'answer_id': 5,\n",
       "    'raw_answer': 'down',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'down'},\n",
       "   {'answer_id': 6,\n",
       "    'raw_answer': 'down',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'down'},\n",
       "   {'answer_id': 7,\n",
       "    'raw_answer': 'down',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'down'},\n",
       "   {'answer_id': 8,\n",
       "    'raw_answer': 'down',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'down'},\n",
       "   {'answer_id': 9,\n",
       "    'raw_answer': 'leave it up',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'leave it up'},\n",
       "   {'answer_id': 10,\n",
       "    'raw_answer': 'leave it up',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer': 'leave it up'}],\n",
       "  'confidence': 4,\n",
       "  'image_object': 'a toilet with the seat up'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ae9f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_ids = [anns[i]['question_id'] for i in range(len(anns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d378b172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5046"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(question_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19ac8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_train = load_prompt(prompt_file='mscoco_train2014_annotations.json', partition=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63b3e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_ids_train = [anns_train[i]['question_id'] for i in range(len(anns_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c142094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9009, 9009)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(question_ids_train)), len(question_ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ecae291",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = question_ids + question_ids_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1669a79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14055, 14055)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(questions)), len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3471c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
